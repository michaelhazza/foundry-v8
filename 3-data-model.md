# Data Model Document: Foundry
**Generated by:** Agent 3 - Data Modeling v17 (Sonnet Optimized)  
**Date:** 2026-01-19  
**Status:** Complete  
**Framework:** Agent Specification Framework v2.1  
**Constitution:** Agent 0 v3.1

---

## CHECKPOINT 1: Entity Analysis

### Extracted Entities

| Entity | Purpose | Source | Key Relationships |
|--------|---------|--------|-------------------|
| organizations | Multi-tenant root entity for customer companies | PRD Multi-tenancy | Has many users, projects |
| users | System users with authentication | PRD User stories | Belongs to organization, has many projects |
| projects | Work containers for dataset preparation workflows | PRD Core entity | Belongs to user, has many sources, datasets |
| sources | Data inputs (files, API connections) | PRD Core entity | Belongs to project, has many processing_jobs |
| source_files | Binary file storage for uploaded files | Architecture File storage | Belongs to source |
| source_configurations | Mapping and de-identification rules per source | PRD Configuration | Belongs to source |
| processing_jobs | Background processing task tracking | Architecture Processing | Belongs to source, produces dataset |
| datasets | Processed output with de-identified data | PRD Core entity | Belongs to source via processing_job |
| api_credentials | Encrypted OAuth tokens for external APIs | Architecture Integration | Belongs to source |
| refresh_tokens | JWT refresh token rotation tracking | Architecture Auth | Belongs to user |

### Entity Relationship Diagram

```
organizations 1──< users 1──< projects 1──< sources 1──< processing_jobs 1──< datasets
                     │                           │
                     │                           ├──< source_files (1:1)
                     │                           ├──< source_configurations (1:1)
                     │                           └──< api_credentials (1:0..1)
                     │
                     └──< refresh_tokens
```

**Key Design Decisions:**
- Organizations as multi-tenant root (single org per user for MVP per AR-004)
- Source has 1:1 with source_files and source_configurations (simpler than polymorphic)
- Processing jobs create datasets (one-to-many for re-processing scenarios)
- Refresh tokens separate table for rotation pattern (AR-009)
- Password reset tokens in users table (AR-010)

**Continuing immediately to entity design...**

---

## CHECKPOINT 2: Complete Schema

### 1. Organizations Table

**Purpose:** Multi-tenant root entity representing customer companies

```typescript
export const organizations = pgTable('organizations', {
  id: serial('id').primaryKey(),
  name: text('name').notNull(),
  slug: text('slug').notNull().unique(), // URL-safe identifier: acme-corp
  status: text('status').notNull().default('active'), // active | suspended | deleted
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Organization = typeof organizations.$inferSelect;
export type NewOrganization = typeof organizations.$inferInsert;
```

**Constraints:**
- `name` NOT NULL (required for identification)
- `slug` UNIQUE (used in URLs, invite flows)
- `status` default 'active' (for account suspension)

**Indexes:**
- Primary key on `id` (automatic)
- Unique index on `slug` (automatic via constraint)

---

### 2. Users Table

**Purpose:** System users with authentication credentials and profile

```typescript
export const users = pgTable('users', {
  id: serial('id').primaryKey(),
  organizationId: integer('organization_id')
    .notNull()
    .references(() => organizations.id, { onDelete: 'cascade' }),
  email: text('email').notNull().unique(),
  passwordHash: text('password_hash').notNull(),
  name: text('name'),
  role: text('role').notNull().default('user'), // admin | user
  status: text('status').notNull().default('active'), // active | invited | suspended
  invitedBy: integer('invited_by').references(() => users.id, { onDelete: 'set null' }),
  passwordResetToken: text('password_reset_token'),
  passwordResetExpires: timestamp('password_reset_expires'),
  lastLoginAt: timestamp('last_login_at'),
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type User = typeof users.$inferSelect;
export type NewUser = typeof users.$inferInsert;
```

**Constraints:**
- `email` UNIQUE, NOT NULL (primary login identifier)
- `organizationId` NOT NULL (every user belongs to one org per AR-004)
- `passwordHash` NOT NULL (no social login for MVP)
- `role` default 'user' (permissions model)
- `status` default 'active' (invitation workflow)
- `passwordResetToken` NULL (only set during reset flow per AR-010)

**Cascade Behaviors:**
- `organizationId` CASCADE (delete users when org deleted)
- `invitedBy` SET NULL (preserve user if inviter deleted)

**Indexes:**
- Primary key on `id`
- Unique index on `email`
- Index on `organizationId` (frequent join)
- Index on `passwordResetToken` (reset flow lookup)

---

### 3. Refresh Tokens Table

**Purpose:** JWT refresh token rotation tracking for secure logout

```typescript
export const refreshTokens = pgTable('refresh_tokens', {
  id: serial('id').primaryKey(),
  userId: integer('user_id')
    .notNull()
    .references(() => users.id, { onDelete: 'cascade' }),
  tokenHash: text('token_hash').notNull().unique(), // SHA-256 hash of token
  expiresAt: timestamp('expires_at').notNull(),
  usedAt: timestamp('used_at'), // NULL = unused, timestamp = consumed
  replacedBy: integer('replaced_by').references(() => refreshTokens.id, { onDelete: 'set null' }),
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type RefreshToken = typeof refreshTokens.$inferSelect;
export type NewRefreshToken = typeof refreshTokens.$inferInsert;
```

**Constraints:**
- `tokenHash` UNIQUE (prevent token reuse)
- `expiresAt` NOT NULL (automatic expiry)
- `usedAt` NULL = unused (rotation detection)
- `replacedBy` tracks token chain (audit trail)

**Cascade Behaviors:**
- `userId` CASCADE (delete tokens when user deleted)
- `replacedBy` SET NULL (preserve token history)

**Indexes:**
- Primary key on `id`
- Unique index on `tokenHash` (lookup during refresh)
- Index on `userId` (cleanup queries)
- Index on `expiresAt` (automatic cleanup job)

**Security Pattern (per AR-009):**
```typescript
// On refresh: invalidate old token, create new one
await db.update(refreshTokens)
  .set({ usedAt: new Date() })
  .where(eq(refreshTokens.id, oldTokenId));
```

---

### 4. Projects Table

**Purpose:** User-owned work containers for dataset preparation workflows

```typescript
export const projects = pgTable('projects', {
  id: serial('id').primaryKey(),
  userId: integer('user_id')
    .notNull()
    .references(() => users.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  description: text('description'),
  status: text('status').notNull().default('active'), // active | archived | deleted
  deletedAt: timestamp('deleted_at'), // Soft delete
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Project = typeof projects.$inferSelect;
export type NewProject = typeof projects.$inferInsert;
```

**Constraints:**
- `userId` NOT NULL (every project owned by user)
- `name` NOT NULL (required for UI)
- `deletedAt` NULL = active (soft delete pattern)

**Cascade Behaviors:**
- `userId` CASCADE (delete projects when user deleted)

**Indexes:**
- Primary key on `id`
- Index on `userId` (user's projects list)
- Index on `deletedAt` (filter deleted projects)

**Soft Delete Pattern:**
```typescript
// Soft delete
await db.update(projects)
  .set({ deletedAt: new Date(), status: 'deleted' })
  .where(eq(projects.id, projectId));

// Query active projects only
const activeProjects = await db
  .select()
  .from(projects)
  .where(and(eq(projects.userId, userId), isNull(projects.deletedAt)));
```

---

### 5. Sources Table

**Purpose:** Data inputs (file uploads or API connections) within projects

```typescript
export const sources = pgTable('sources', {
  id: serial('id').primaryKey(),
  projectId: integer('project_id')
    .notNull()
    .references(() => projects.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  type: text('type').notNull(), // file | teamwork_desk | api (extensible)
  status: text('status').notNull().default('pending'), // pending | configured | processing | ready | error
  metadata: text('metadata'), // JSON: { originalFilename, fileType, recordCount, etc. }
  deletedAt: timestamp('deleted_at'), // Soft delete
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Source = typeof sources.$inferSelect;
export type NewSource = typeof sources.$inferInsert;
```

**Constraints:**
- `projectId` NOT NULL (sources belong to projects)
- `name` NOT NULL (user-provided or auto-generated)
- `type` NOT NULL (determines processing pipeline)
- `status` tracks source lifecycle
- `metadata` JSON string (flexible source-specific data)

**Cascade Behaviors:**
- `projectId` CASCADE (delete sources when project deleted)

**Indexes:**
- Primary key on `id`
- Index on `projectId` (project's sources list)
- Index on `status` (filter by processing state)
- Index on `deletedAt` (soft delete filter)

---

### 6. Source Files Table

**Purpose:** Binary file storage for uploaded files (1:1 with file sources)

```typescript
export const sourceFiles = pgTable('source_files', {
  id: serial('id').primaryKey(),
  sourceId: integer('source_id')
    .notNull()
    .references(() => sources.id, { onDelete: 'cascade' })
    .unique(), // 1:1 relationship
  filename: text('filename').notNull(),
  mimeType: text('mime_type').notNull(),
  fileSize: integer('file_size').notNull(), // bytes
  fileData: text('file_data').notNull(), // Base64-encoded binary data
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type SourceFile = typeof sourceFiles.$inferSelect;
export type NewSourceFile = typeof sourceFiles.$inferInsert;
```

**Constraints:**
- `sourceId` UNIQUE (1:1 with source)
- `fileData` stores Base64 blob (per AR-001 database storage)
- `fileSize` for validation (100MB limit per Architecture)

**Cascade Behaviors:**
- `sourceId` CASCADE (delete file when source deleted)

**Indexes:**
- Primary key on `id`
- Unique index on `sourceId` (1:1 lookup)

**Storage Decision (AR-001):**
Files stored in database due to Replit's ephemeral filesystem. Base64 encoding for text storage. If performance issues emerge, migrate to object storage (S3/GCS) post-MVP.

**Size Validation:**
```typescript
const MAX_FILE_SIZE = 100 * 1024 * 1024; // 100MB
if (fileSize > MAX_FILE_SIZE) {
  throw new Error('File exceeds 100MB limit');
}
```

---

### 7. Source Configurations Table

**Purpose:** Mapping and de-identification rules for each source (1:1 relationship)

```typescript
export const sourceConfigurations = pgTable('source_configurations', {
  id: serial('id').primaryKey(),
  sourceId: integer('source_id')
    .notNull()
    .references(() => sources.id, { onDelete: 'cascade' })
    .unique(), // 1:1 relationship
  targetSchema: text('target_schema').notNull(), // JSON: { fields: [{ name, type, required }] }
  fieldMappings: text('field_mappings').notNull(), // JSON: { source_field -> target_field }
  deidentificationRules: text('deidentification_rules').notNull(), // JSON: [{ field, action, pattern }]
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type SourceConfiguration = typeof sourceConfigurations.$inferSelect;
export type NewSourceConfiguration = typeof sourceConfigurations.$inferInsert;
```

**Constraints:**
- `sourceId` UNIQUE (1:1 with source)
- All configuration fields stored as JSON strings (flexible schema)

**Cascade Behaviors:**
- `sourceId` CASCADE (delete config when source deleted)

**Indexes:**
- Primary key on `id`
- Unique index on `sourceId` (1:1 lookup)

**JSON Schema Examples:**

```typescript
// targetSchema format
{
  "name": "support_conversation",
  "fields": [
    { "name": "ticket_id", "type": "string", "required": true },
    { "name": "customer_message", "type": "text", "required": true },
    { "name": "agent_response", "type": "text", "required": false }
  ]
}

// fieldMappings format
{
  "Ticket ID": "ticket_id",
  "Customer Email Body": "customer_message",
  "Agent Reply": "agent_response"
}

// deidentificationRules format
[
  { "field": "customer_message", "action": "redact_email", "pattern": "email" },
  { "field": "customer_message", "action": "redact_phone", "pattern": "phone" },
  { "field": "customer_name", "action": "tokenize", "pattern": null }
]
```

---

### 8. API Credentials Table

**Purpose:** Encrypted OAuth tokens and API keys for external service integrations

```typescript
export const apiCredentials = pgTable('api_credentials', {
  id: serial('id').primaryKey(),
  sourceId: integer('source_id')
    .notNull()
    .references(() => sources.id, { onDelete: 'cascade' })
    .unique(), // 1:1 relationship (one credential set per API source)
  provider: text('provider').notNull(), // teamwork_desk | salesforce | etc.
  encryptedAccessToken: text('encrypted_access_token').notNull(),
  encryptedRefreshToken: text('encrypted_refresh_token'),
  tokenExpiresAt: timestamp('token_expires_at'),
  metadata: text('metadata'), // JSON: { domain, userId, etc. }
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type ApiCredential = typeof apiCredentials.$inferSelect;
export type NewApiCredential = typeof apiCredentials.$inferInsert;
```

**Constraints:**
- `sourceId` UNIQUE (1:1 with API sources)
- `provider` NOT NULL (identifies integration type)
- All tokens encrypted with AES-256-GCM (per AR-011)

**Cascade Behaviors:**
- `sourceId` CASCADE (delete credentials when source deleted)

**Indexes:**
- Primary key on `id`
- Unique index on `sourceId` (1:1 lookup)

**Encryption Pattern (per AR-011):**
```typescript
import crypto from 'crypto';
import { env } from '../config/env';

const ALGORITHM = 'aes-256-gcm';
const KEY = Buffer.from(env.ENCRYPTION_KEY, 'hex'); // 32-byte key

export function encrypt(plaintext: string): string {
  const iv = crypto.randomBytes(16);
  const cipher = crypto.createCipheriv(ALGORITHM, KEY, iv);
  let encrypted = cipher.update(plaintext, 'utf8', 'hex');
  encrypted += cipher.final('hex');
  const authTag = cipher.getAuthTag();
  return `${iv.toString('hex')}:${authTag.toString('hex')}:${encrypted}`;
}

export function decrypt(ciphertext: string): string {
  const [ivHex, authTagHex, encrypted] = ciphertext.split(':');
  const iv = Buffer.from(ivHex, 'hex');
  const authTag = Buffer.from(authTagHex, 'hex');
  const decipher = crypto.createDecipheriv(ALGORITHM, KEY, iv);
  decipher.setAuthTag(authTag);
  let decrypted = decipher.update(encrypted, 'hex', 'utf8');
  decrypted += decipher.final('utf8');
  return decrypted;
}
```

---

### 9. Processing Jobs Table

**Purpose:** Background processing task tracking with progress and status

```typescript
export const processingJobs = pgTable('processing_jobs', {
  id: serial('id').primaryKey(),
  sourceId: integer('source_id')
    .notNull()
    .references(() => sources.id, { onDelete: 'cascade' }),
  status: text('status').notNull().default('pending'), // pending | running | completed | failed
  stage: text('stage'), // parsing | detecting_pii | deidentifying | mapping | complete
  progress: integer('progress').notNull().default(0), // 0-100
  recordsProcessed: integer('records_processed').notNull().default(0),
  totalRecords: integer('total_records'),
  errorMessage: text('error_message'),
  startedAt: timestamp('started_at'),
  completedAt: timestamp('completed_at'),
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type ProcessingJob = typeof processingJobs.$inferSelect;
export type NewProcessingJob = typeof processingJobs.$inferInsert;
```

**Constraints:**
- `sourceId` NOT NULL (jobs belong to sources)
- `status` tracks job lifecycle
- `stage` provides granular progress info
- `progress` 0-100 for UI progress bars
- `errorMessage` stores failure details

**Cascade Behaviors:**
- `sourceId` CASCADE (delete jobs when source deleted)

**Indexes:**
- Primary key on `id`
- Index on `sourceId` (source's jobs list)
- Index on `status` (filter active jobs)
- Composite index on `(status, startedAt)` (queue processing)

**Processing Pattern:**
```typescript
// Create job
const [job] = await db.insert(processingJobs)
  .values({ sourceId, status: 'pending' })
  .returning();

// Update progress (called by processing engine)
await db.update(processingJobs)
  .set({ 
    status: 'running',
    stage: 'detecting_pii', 
    progress: 50,
    recordsProcessed: 500,
    startedAt: new Date(),
  })
  .where(eq(processingJobs.id, jobId));

// Complete job
await db.update(processingJobs)
  .set({ 
    status: 'completed',
    stage: 'complete',
    progress: 100,
    completedAt: new Date(),
  })
  .where(eq(processingJobs.id, jobId));
```

---

### 10. Datasets Table

**Purpose:** Processed output with de-identified, structured data

```typescript
export const datasets = pgTable('datasets', {
  id: serial('id').primaryKey(),
  processingJobId: integer('processing_job_id')
    .notNull()
    .references(() => processingJobs.id, { onDelete: 'cascade' }),
  name: text('name').notNull(),
  format: text('format').notNull().default('jsonl'), // jsonl | csv | json
  recordCount: integer('record_count').notNull(),
  fileSize: integer('file_size').notNull(), // bytes
  storageKey: text('storage_key').notNull(), // database: id, object storage: S3 key
  dataContent: text('data_content'), // JSON array or Base64 (for database storage)
  downloadUrl: text('download_url'), // Signed URL or /api/datasets/:id/download
  metadata: text('metadata'), // JSON: { piiFieldsRedacted, transformationsSummary }
  expiresAt: timestamp('expires_at'), // NULL = no expiry, timestamp = auto-cleanup
  createdAt: timestamp('created_at').defaultNow().notNull(),
  updatedAt: timestamp('updated_at').defaultNow().notNull(),
});

export type Dataset = typeof datasets.$inferSelect;
export type NewDataset = typeof datasets.$inferInsert;
```

**Constraints:**
- `processingJobId` NOT NULL (datasets produced by jobs)
- `name` auto-generated or user-provided
- `format` determines output structure
- `dataContent` stores actual dataset (database storage)
- `expiresAt` for auto-cleanup (post-MVP feature)

**Cascade Behaviors:**
- `processingJobId` CASCADE (delete datasets when job deleted)

**Indexes:**
- Primary key on `id`
- Index on `processingJobId` (job's output datasets)
- Index on `expiresAt` (cleanup job)

**Dataset Generation Pattern:**
```typescript
// After processing completes
const dataContent = JSON.stringify(deidentifiedRecords);
const [dataset] = await db.insert(datasets)
  .values({
    processingJobId: job.id,
    name: `${source.name} - ${new Date().toISOString()}`,
    format: 'jsonl',
    recordCount: deidentifiedRecords.length,
    fileSize: Buffer.byteLength(dataContent),
    storageKey: `dataset-${uuid()}`,
    dataContent,
    downloadUrl: `/api/datasets/${datasetId}/download`,
    metadata: JSON.stringify({
      piiFieldsRedacted: ['email', 'phone'],
      transformationsSummary: 'Redacted 150 emails, 45 phone numbers',
    }),
  })
  .returning();
```

---

## 3. Relationship Summary

### Foreign Key Definitions

| Table | Foreign Key | References | Cascade Behavior | Rationale |
|-------|-------------|------------|------------------|-----------|
| users | organization_id | organizations.id | CASCADE | Delete users when org deleted |
| users | invited_by | users.id | SET NULL | Preserve user if inviter deleted |
| refresh_tokens | user_id | users.id | CASCADE | Delete tokens when user deleted |
| refresh_tokens | replaced_by | refresh_tokens.id | SET NULL | Preserve token history |
| projects | user_id | users.id | CASCADE | Delete projects when user deleted |
| sources | project_id | projects.id | CASCADE | Delete sources when project deleted |
| source_files | source_id | sources.id | CASCADE | Delete file when source deleted |
| source_configurations | source_id | sources.id | CASCADE | Delete config when source deleted |
| api_credentials | source_id | sources.id | CASCADE | Delete credentials when source deleted |
| processing_jobs | source_id | sources.id | CASCADE | Delete jobs when source deleted |
| datasets | processing_job_id | processing_jobs.id | CASCADE | Delete datasets when job deleted |

### Cardinality Matrix

| Relationship | Type | Notes |
|--------------|------|-------|
| Organization → Users | 1:N | Single org per user (AR-004) |
| User → Projects | 1:N | Users own multiple projects |
| Project → Sources | 1:N | Projects contain multiple sources |
| Source → Source File | 1:0..1 | Only file sources have files |
| Source → Source Configuration | 1:1 | Every source needs config |
| Source → API Credentials | 1:0..1 | Only API sources have credentials |
| Source → Processing Jobs | 1:N | Re-process same source multiple times |
| Processing Job → Datasets | 1:N | Job produces multiple format outputs |
| User → Refresh Tokens | 1:N | Multiple devices/sessions |

### Junction Tables

**None required.** All relationships are direct foreign keys. Multi-org membership would require `organization_memberships` junction table (post-MVP per AR-004).

---

## 4. Index Strategy

### Primary Indexes (Automatic)

All tables have automatic primary key indexes on `id` column.

### Foreign Key Indexes (Required)

```typescript
// users table
CREATE INDEX idx_users_organization_id ON users(organization_id);
CREATE INDEX idx_users_invited_by ON users(invited_by);

// refresh_tokens table
CREATE INDEX idx_refresh_tokens_user_id ON refresh_tokens(user_id);

// projects table
CREATE INDEX idx_projects_user_id ON projects(user_id);

// sources table
CREATE INDEX idx_sources_project_id ON sources(project_id);

// processing_jobs table
CREATE INDEX idx_processing_jobs_source_id ON processing_jobs(source_id);

// datasets table
CREATE INDEX idx_datasets_processing_job_id ON datasets(processing_job_id);
```

### Unique Constraint Indexes (Automatic)

```typescript
// organizations
CREATE UNIQUE INDEX organizations_slug_key ON organizations(slug);

// users
CREATE UNIQUE INDEX users_email_key ON users(email);

// refresh_tokens
CREATE UNIQUE INDEX refresh_tokens_token_hash_key ON refresh_tokens(token_hash);

// source_files (1:1 relationship)
CREATE UNIQUE INDEX source_files_source_id_key ON source_files(source_id);

// source_configurations (1:1 relationship)
CREATE UNIQUE INDEX source_configurations_source_id_key ON source_configurations(source_id);

// api_credentials (1:1 relationship)
CREATE UNIQUE INDEX api_credentials_source_id_key ON api_credentials(source_id);
```

### Query-Driven Indexes

```typescript
// Password reset lookup
CREATE INDEX idx_users_password_reset_token ON users(password_reset_token) 
  WHERE password_reset_token IS NOT NULL;

// Active projects filter (soft delete)
CREATE INDEX idx_projects_deleted_at ON projects(deleted_at);

// Active sources filter (soft delete)
CREATE INDEX idx_sources_deleted_at ON sources(deleted_at);

// Processing queue (concurrent job limit per AR-012)
CREATE INDEX idx_processing_jobs_status_started ON processing_jobs(status, started_at);

// Dataset cleanup
CREATE INDEX idx_datasets_expires_at ON datasets(expires_at) 
  WHERE expires_at IS NOT NULL;

// Token cleanup
CREATE INDEX idx_refresh_tokens_expires_at ON refresh_tokens(expires_at);
```

### Composite Indexes for Common Queries

```typescript
// User's active projects
CREATE INDEX idx_projects_user_deleted ON projects(user_id, deleted_at);

// Project's active sources
CREATE INDEX idx_sources_project_status ON sources(project_id, status);

// Source's recent jobs
CREATE INDEX idx_processing_jobs_source_created ON processing_jobs(source_id, created_at DESC);
```

---

## 5. Query Patterns

### Authentication Queries

```typescript
// Login
const [user] = await db
  .select()
  .from(users)
  .where(eq(users.email, email))
  .limit(1);

// Create refresh token
const [token] = await db
  .insert(refreshTokens)
  .values({
    userId: user.id,
    tokenHash: hashToken(refreshToken),
    expiresAt: new Date(Date.now() + 7 * 24 * 60 * 60 * 1000), // 7 days
  })
  .returning();

// Refresh token validation
const [token] = await db
  .select()
  .from(refreshTokens)
  .where(
    and(
      eq(refreshTokens.tokenHash, tokenHash),
      isNull(refreshTokens.usedAt),
      gt(refreshTokens.expiresAt, new Date())
    )
  )
  .limit(1);
```

### Project Listing (with Source Count)

```typescript
// User's projects with source counts (avoid N+1)
const projectsWithCounts = await db
  .select({
    project: projects,
    sourceCount: count(sources.id),
  })
  .from(projects)
  .leftJoin(sources, eq(sources.projectId, projects.id))
  .where(
    and(
      eq(projects.userId, userId),
      isNull(projects.deletedAt)
    )
  )
  .groupBy(projects.id);
```

### Source Details (with Latest Job)

```typescript
// Source with latest processing job (avoid N+1)
const sourceWithJob = await db
  .select({
    source: sources,
    latestJob: processingJobs,
  })
  .from(sources)
  .leftJoin(
    processingJobs,
    and(
      eq(processingJobs.sourceId, sources.id),
      // Subquery to get latest job per source
      eq(
        processingJobs.id,
        db
          .select({ maxId: max(processingJobs.id) })
          .from(processingJobs)
          .where(eq(processingJobs.sourceId, sources.id))
      )
    )
  )
  .where(eq(sources.id, sourceId))
  .limit(1);
```

### Processing Job Progress Polling

```typescript
// Get job progress (called every 2 seconds during processing per AR-005)
const [job] = await db
  .select({
    id: processingJobs.id,
    status: processingJobs.status,
    stage: processingJobs.stage,
    progress: processingJobs.progress,
    recordsProcessed: processingJobs.recordsProcessed,
    totalRecords: processingJobs.totalRecords,
  })
  .from(processingJobs)
  .where(eq(processingJobs.id, jobId))
  .limit(1);
```

### Dataset Download

```typescript
// Fetch dataset for download
const [dataset] = await db
  .select({
    id: datasets.id,
    name: datasets.name,
    format: datasets.format,
    dataContent: datasets.dataContent,
  })
  .from(datasets)
  .innerJoin(processingJobs, eq(datasets.processingJobId, processingJobs.id))
  .innerJoin(sources, eq(processingJobs.sourceId, sources.id))
  .innerJoin(projects, eq(sources.projectId, projects.id))
  .where(
    and(
      eq(datasets.id, datasetId),
      eq(projects.userId, userId) // Security: ensure user owns dataset
    )
  )
  .limit(1);
```

### Transaction Example: Create Source with Configuration

```typescript
import { db } from './index';

await db.transaction(async (tx) => {
  // Create source
  const [source] = await tx
    .insert(sources)
    .values({
      projectId,
      name: fileName,
      type: 'file',
      status: 'pending',
    })
    .returning();

  // Upload file
  await tx
    .insert(sourceFiles)
    .values({
      sourceId: source.id,
      filename: fileName,
      mimeType: fileType,
      fileSize: fileData.length,
      fileData: fileData.toString('base64'),
    });

  // Create default configuration
  await tx
    .insert(sourceConfigurations)
    .values({
      sourceId: source.id,
      targetSchema: JSON.stringify({ fields: [] }),
      fieldMappings: JSON.stringify({}),
      deidentificationRules: JSON.stringify([]),
    });

  return source;
});
```

---

## 6. Migration Strategy

### Initial Migration Approach

**Tool:** Drizzle Kit (declarative migrations)

**Migration Commands:**
```bash
# Generate migration from schema
npm run db:generate

# Apply migrations to database
npm run db:migrate

# Seed database with test data
npm run db:seed
```

**Migration Sequence:**
1. Create `organizations` table
2. Create `users` table (depends on organizations)
3. Create `refresh_tokens` table (depends on users)
4. Create `projects` table (depends on users)
5. Create `sources` table (depends on projects)
6. Create `source_files` table (depends on sources)
7. Create `source_configurations` table (depends on sources)
8. Create `api_credentials` table (depends on sources)
9. Create `processing_jobs` table (depends on sources)
10. Create `datasets` table (depends on processing_jobs)

### Drizzle Configuration

```typescript
// drizzle.config.ts
import type { Config } from 'drizzle-kit';
import { env } from './server/config/env';

export default {
  schema: './server/db/schema.ts',
  out: './drizzle',
  driver: 'pg',
  dbCredentials: {
    connectionString: env.DATABASE_URL,
  },
  verbose: true,
  strict: true,
} satisfies Config;
```

### Seed Data Requirements

```typescript
// server/db/seed.ts
import { db } from './index';
import { organizations, users, projects, sources } from './schema';
import bcrypt from 'bcryptjs';

async function seed() {
  console.log('Seeding database...');

  // Create test organization
  const [org] = await db
    .insert(organizations)
    .values({
      name: 'Acme Corporation',
      slug: 'acme-corp',
      status: 'active',
    })
    .returning();

  // Create test user
  const [user] = await db
    .insert(users)
    .values({
      organizationId: org.id,
      email: 'test@acme.com',
      passwordHash: await bcrypt.hash('password123', 10),
      name: 'Test User',
      role: 'admin',
      status: 'active',
    })
    .returning();

  // Create test project
  const [project] = await db
    .insert(projects)
    .values({
      userId: user.id,
      name: 'Support Conversations',
      description: 'Customer support ticket analysis',
      status: 'active',
    })
    .returning();

  console.log('Seed complete:', { org, user, project });
}

seed()
  .catch((err) => {
    console.error('Seed failed:', err);
    process.exit(1);
  })
  .finally(() => process.exit(0));
```

### Rollback Procedures

Drizzle Kit does not auto-generate rollback migrations. Manual rollback:

```sql
-- Drop tables in reverse dependency order
DROP TABLE IF EXISTS datasets;
DROP TABLE IF EXISTS processing_jobs;
DROP TABLE IF EXISTS api_credentials;
DROP TABLE IF EXISTS source_configurations;
DROP TABLE IF EXISTS source_files;
DROP TABLE IF EXISTS sources;
DROP TABLE IF EXISTS projects;
DROP TABLE IF EXISTS refresh_tokens;
DROP TABLE IF EXISTS users;
DROP TABLE IF EXISTS organizations;
```

---

## 7. Type Exports

```typescript
// server/db/schema.ts - Export all inferred types

// Organizations
export type Organization = typeof organizations.$inferSelect;
export type NewOrganization = typeof organizations.$inferInsert;

// Users
export type User = typeof users.$inferSelect;
export type NewUser = typeof users.$inferInsert;

// Refresh Tokens
export type RefreshToken = typeof refreshTokens.$inferSelect;
export type NewRefreshToken = typeof refreshTokens.$inferInsert;

// Projects
export type Project = typeof projects.$inferSelect;
export type NewProject = typeof projects.$inferInsert;

// Sources
export type Source = typeof sources.$inferSelect;
export type NewSource = typeof sources.$inferInsert;

// Source Files
export type SourceFile = typeof sourceFiles.$inferSelect;
export type NewSourceFile = typeof sourceFiles.$inferInsert;

// Source Configurations
export type SourceConfiguration = typeof sourceConfigurations.$inferSelect;
export type NewSourceConfiguration = typeof sourceConfigurations.$inferInsert;

// API Credentials
export type ApiCredential = typeof apiCredentials.$inferSelect;
export type NewApiCredential = typeof apiCredentials.$inferInsert;

// Processing Jobs
export type ProcessingJob = typeof processingJobs.$inferSelect;
export type NewProcessingJob = typeof processingJobs.$inferInsert;

// Datasets
export type Dataset = typeof datasets.$inferSelect;
export type NewDataset = typeof datasets.$inferInsert;
```

---

## Document Validation

### Completeness Checklist

- [x] All PRD entities represented (organizations, users, projects, sources, datasets)
- [x] All relationships defined with cascade behavior (10 foreign keys with explicit ON DELETE)
- [x] Foreign keys indexed (all 10 foreign key indexes specified)
- [x] Audit columns on all tables (createdAt, updatedAt on all 10 tables)
- [x] Migration scripts specified (Drizzle Kit setup, seed script)
- [x] Type exports defined (10 entity types with Select/Insert variants)

### Prompt Maintenance Contract

If this prompt is edited, you MUST:
1. Update version history with changes and `Hygiene Gate: PASS`
2. Re-run Prompt Hygiene Gate checks (Constitution Section L)
3. Confirm clean encoding (no mojibake/non-ASCII artifacts)
4. Verify no global rule restatements (reference Constitution instead)

Failed checks invalidate prompt update.

### Prompt Hygiene Gate (Constitution Section L)

- [x] Framework Version header present and correct (v2.1)
- [x] Encoding scan: No non-ASCII artifact tokens
- [x] Inheritance references Constitution v3.1
- [x] No full global rule restatements (uses "Per Constitution Section X")

### Entity Coverage

| PRD Entity | Schema Table | Status |
|------------|--------------|--------|
| Organizations | organizations | âœ" Complete |
| Users | users | âœ" Complete |
| Projects | projects | âœ" Complete |
| Sources | sources | âœ" Complete |
| Source Files | source_files | âœ" Complete |
| Source Configurations | source_configurations | âœ" Complete |
| Processing Jobs | processing_jobs | âœ" Complete |
| Datasets | datasets | âœ" Complete |
| API Credentials | api_credentials | âœ" Complete |
| Refresh Tokens | refresh_tokens | âœ" Complete |

**Additional Tables (Architectural Requirements):**
- `refresh_tokens` - JWT rotation (AR-009)
- `password_reset_token` - In users table (AR-010)
- `api_credentials` - Encrypted OAuth tokens (AR-011)

### Document Status: COMPLETE

---

## Downstream Agent Handoff Brief

### Agent 4: API Contract

**Entity Operations Needed:**

| Entity | CRUD Operations | Notes |
|--------|-----------------|-------|
| organizations | Read only | Single org per user (MVP) |
| users | Create, Read, Update | Registration, profile, password reset |
| projects | Create, Read, Update, Delete | Soft delete with deletedAt |
| sources | Create, Read, Update, Delete | Soft delete with deletedAt |
| source_files | Create, Read | Binary data via Base64 |
| source_configurations | Create, Read, Update | JSON configuration objects |
| api_credentials | Create, Read, Update, Delete | Encrypted tokens |
| processing_jobs | Create, Read | Background processing status |
| datasets | Read | Download endpoint |

**Relationship Traversal Patterns:**
- User → Projects → Sources → Processing Jobs → Datasets (navigation drill-down)
- Source → Latest Processing Job (status polling)
- Processing Job → Datasets (download list)

**Pagination Requirements:**
- Projects list: 20 per page
- Sources list: 50 per page
- Processing jobs history: 20 per page
- Datasets list: 20 per page

**Security Filters (ALL queries):**
- Verify user owns resource via organization/user/project hierarchy
- Never expose other org's data
- Apply soft delete filters (deletedAt IS NULL)

---

### Agent 5: UI/UX Specification

**Data Shapes for Forms:**

```typescript
// Create Project Form
type CreateProjectInput = {
  name: string;
  description?: string;
};

// Upload File Form
type UploadFileInput = {
  projectId: number;
  file: File; // Max 100MB
  name?: string; // Auto-generated from filename
};

// Configure Source Form
type ConfigureSourceInput = {
  targetSchema: {
    name: string;
    fields: Array<{ name: string; type: string; required: boolean }>;
  };
  fieldMappings: Record<string, string>; // source_field -> target_field
  deidentificationRules: Array<{
    field: string;
    action: 'redact' | 'tokenize' | 'remove';
    pattern?: string;
  }>;
};

// Connect Teamwork Desk Form
type ConnectTeamworkDeskInput = {
  projectId: number;
  domain: string; // e.g., acme.teamwork.com
  // OAuth flow: redirects to Teamwork, callback stores tokens
};
```

**List/Detail Patterns:**

```typescript
// Projects List Item
type ProjectListItem = {
  id: number;
  name: string;
  description?: string;
  sourceCount: number;
  datasetCount: number;
  lastUpdated: Date;
};

// Source Detail View
type SourceDetail = {
  id: number;
  name: string;
  type: 'file' | 'teamwork_desk' | 'api';
  status: 'pending' | 'configured' | 'processing' | 'ready' | 'error';
  latestJob?: {
    status: string;
    stage: string;
    progress: number;
    recordsProcessed: number;
    totalRecords: number;
  };
  configuration?: {
    targetSchema: any;
    fieldMappings: any;
    deidentificationRules: any;
  };
  datasets: Array<{
    id: number;
    name: string;
    format: string;
    recordCount: number;
    createdAt: Date;
  }>;
};
```

**Relationship Displays:**
- Breadcrumbs: Organization > User > Project > Source
- Nested lists: Projects contain Sources, Sources contain Datasets
- Progress indicators: Processing Job status with real-time updates (polling)

---

### Agent 6: Implementation Orchestrator

**File Structure:**

```
server/
├── db/
│   ├── index.ts           # Database connection, postgres-js setup
│   ├── schema.ts          # All table definitions, type exports
│   ├── seed.ts            # Seed script
│   └── migrations/        # Drizzle Kit auto-generated
├── config/
│   └── env.ts             # DATABASE_URL, ENCRYPTION_KEY
└── utils/
    └── encryption.ts      # encrypt/decrypt functions
```

**Database Connection Module (server/db/index.ts):**

```typescript
import postgres from 'postgres';
import { drizzle } from 'drizzle-orm/postgres-js';
import * as schema from './schema';
import { env } from '../config/env';

const sql = postgres(env.DATABASE_URL, {
  max: 10,              // Connection pool size
  idle_timeout: 20,     // Close idle connections after 20s
  connect_timeout: 10,  // Connection timeout
});

export const db = drizzle(sql, { schema });

export async function closeDatabase() {
  await sql.end();
}
```

**Schema File (server/db/schema.ts):**
All table definitions from this document, plus type exports.

**Migration Commands:**
```json
// package.json scripts
{
  "scripts": {
    "db:generate": "drizzle-kit generate:pg",
    "db:migrate": "drizzle-kit push:pg",
    "db:seed": "tsx server/db/seed.ts",
    "db:studio": "drizzle-kit studio"
  }
}
```

**Type Imports:**
```typescript
// In controllers/services
import type { User, NewUser, Project, NewProject } from '../db/schema';
import { db } from '../db';
import { users, projects } from '../db/schema';
```

**Critical Patterns:**
- Always use Core Select API (`db.select().from()`)
- Never use Query API (`db.query.users.findFirst()`)
- Apply soft delete filters: `and(eq(...), isNull(table.deletedAt))`
- Use transactions for multi-table operations
- Avoid N+1 queries: use JOINs, not loops

---

### Agent 7: QA & Deployment

**Seed Data for Testing:**

```typescript
// Minimal test data per seed.ts
- 1 organization (Acme Corporation)
- 1 admin user (test@acme.com, password: password123)
- 1 project (Support Conversations)
- 0 sources (created via UI during testing)
```

**Migration Verification Steps:**

1. **Fresh Database:**
```bash
npm run db:generate  # Generate migrations
npm run db:migrate   # Apply migrations
npm run db:seed      # Load test data
```

2. **Schema Verification:**
```sql
-- Check all tables exist
SELECT table_name FROM information_schema.tables 
WHERE table_schema = 'public';

-- Verify foreign keys
SELECT constraint_name, table_name 
FROM information_schema.table_constraints 
WHERE constraint_type = 'FOREIGN KEY';

-- Check indexes
SELECT indexname, tablename FROM pg_indexes 
WHERE schemaname = 'public';
```

3. **Data Integrity Tests:**
- Insert user without organization → should fail (NOT NULL constraint)
- Delete organization → should cascade to users
- Delete user → should cascade to projects, refresh_tokens
- Insert duplicate email → should fail (UNIQUE constraint)
- Soft delete project → deletedAt set, project remains in DB

**Environment Variables Required:**
```bash
DATABASE_URL=postgresql://user:pass@host:5432/foundry
ENCRYPTION_KEY=<32-byte hex key for AES-256-GCM>
```

**Deployment Checklist:**
- [ ] DATABASE_URL configured in Replit secrets
- [ ] ENCRYPTION_KEY generated and stored securely
- [ ] Migrations run successfully (`npm run db:migrate`)
- [ ] Seed data loaded (`npm run db:seed`) for testing
- [ ] Connection pool settings verified (max: 10, idle_timeout: 20)
- [ ] postgres-js driver (NOT @neondatabase/serverless)
- [ ] Graceful shutdown handler closes database (`closeDatabase()`)

---

## ASSUMPTION REGISTER

### AR-001: File Storage in Database (INHERITED FROM PRD)

- **Type:** ASSUMPTION
- **Source Gap:** Executive brief doesn't specify how files are stored (database, filesystem, object storage)
- **Assumption Made:** Files stored in database as Base64 blobs due to Replit's ephemeral filesystem constraint
- **Impact if Wrong:** Database performance issues with large files (100MB), may need to refactor to object storage (S3, GCS) if database storage proves problematic
- **Data Model Decision:** `source_files` table with `file_data` column storing Base64-encoded binary data
- **Migration Path:** If performance issues: add `storage_type` enum ('database' | 's3'), migrate large files to S3, add S3 client library
- **Status:** UNRESOLVED (requires performance testing)
- **Owner:** Agent 6 (Implementation Orchestrator)
- **Date:** 2026-01-19

### AR-004: Multi-Organisation Membership (INHERITED FROM PRD)

- **Type:** ASSUMPTION
- **Source Gap:** Executive brief doesn't clarify if users can belong to multiple organisations
- **Assumption Made:** Single organisation per user for MVP (simpler auth context, no organisation switcher needed)
- **Impact if Wrong:** Users who consult for multiple companies cannot access all their projects in one account; may need to add organisation switcher UI and multi-tenancy auth
- **Data Model Decision:** `users.organization_id` as NOT NULL foreign key (one-to-one relationship); no junction table
- **Migration Path:** If multi-org needed post-MVP: create `organization_memberships(user_id, organization_id, role)` junction table, add organization switcher to JWT context
- **Status:** ACCEPTED (appropriate for MVP simplicity)
- **Owner:** Human (Product Decision)
- **Date:** 2026-01-19

### AR-009: Refresh Token Storage (INHERITED FROM ARCHITECTURE)

- **Type:** ASSUMPTION
- **Source Gap:** Constitution specifies JWT tokens but doesn't specify refresh token storage mechanism
- **Assumption Made:** Store refresh tokens in database table (`refresh_tokens`) with `user_id`, `token_hash`, `expires_at`; rotate on use
- **Impact if Wrong:** Refresh tokens cannot be invalidated (logout doesn't work); token reuse attacks possible
- **Data Model Decision:** `refresh_tokens` table with token rotation pattern (`usedAt`, `replacedBy` columns for audit trail)
- **Status:** RESOLVED (database storage with rotation specified)
- **Owner:** Agent 3 (Data Modeling) - COMPLETE
- **Date:** 2026-01-19

### AR-010: Password Reset Token Storage (INHERITED FROM ARCHITECTURE)

- **Type:** ASSUMPTION
- **Source Gap:** Password reset flow mentioned but token storage not specified
- **Assumption Made:** Store reset tokens in users table (`password_reset_token`, `password_reset_expires`); single-use, 1-hour expiry
- **Impact if Wrong:** Password reset doesn't work; tokens can be reused; security vulnerability
- **Data Model Decision:** Added `password_reset_token` and `password_reset_expires` columns to `users` table; invalidate after successful reset
- **Status:** RESOLVED (users table columns specified)
- **Owner:** Agent 3 (Data Modeling) - COMPLETE
- **Date:** 2026-01-19

### AR-011: API Credential Encryption (INHERITED FROM ARCHITECTURE)

- **Type:** ASSUMPTION
- **Source Gap:** Teamwork OAuth tokens mentioned but encryption not specified
- **Assumption Made:** Encrypt OAuth tokens with AES-256-GCM before storing in `api_credentials` table; use ENCRYPTION_KEY environment variable
- **Impact if Wrong:** OAuth tokens stored in plain text; security vulnerability if database compromised
- **Data Model Decision:** `api_credentials` table with `encrypted_access_token`, `encrypted_refresh_token` columns; encryption utility documented
- **Status:** RESOLVED (AES-256-GCM encryption specified, utility code provided)
- **Owner:** Agent 6 (Implementation Orchestrator) for utility implementation
- **Date:** 2026-01-19

### AR-013: Processing Job Retention

- **Type:** ASSUMPTION
- **Source Gap:** PRD doesn't specify how long to retain processing job history
- **Assumption Made:** Retain all processing jobs indefinitely for audit trail (no auto-cleanup)
- **Impact if Wrong:** Database bloat over time; may need retention policy or archival strategy
- **Data Model Decision:** No cleanup scheduled, no `expires_at` on `processing_jobs` table
- **Proposed Resolution:** Monitor database size post-launch; add retention policy if needed (e.g., archive jobs older than 90 days)
- **Status:** UNRESOLVED (MVP keeps all jobs)
- **Owner:** Agent 7 (QA & Deployment) for monitoring
- **Date:** 2026-01-19

### AR-014: Dataset Expiry/Cleanup

- **Type:** ASSUMPTION
- **Source Gap:** PRD doesn't specify dataset retention policy
- **Assumption Made:** Datasets never expire by default (`expires_at` NULL); manual cleanup only
- **Impact if Wrong:** Database storage costs grow indefinitely as datasets accumulate
- **Data Model Decision:** Added `expires_at` column to `datasets` table for future auto-cleanup (NULL = no expiry)
- **Proposed Resolution:** Post-MVP: add UI option "Auto-delete after 30 days" when creating dataset; background job deletes expired datasets
- **Status:** UNRESOLVED (MVP keeps all datasets)
- **Owner:** Human (Product Decision) for retention policy
- **Date:** 2026-01-19

### AR-015: Soft Delete vs Hard Delete Strategy

- **Type:** ASSUMPTION
- **Source Gap:** PRD mentions delete operations but doesn't specify soft vs hard delete
- **Assumption Made:** Soft delete for `projects` and `sources` (user-recoverable); hard delete for other entities (system-managed)
- **Impact if Wrong:** Users cannot recover accidentally deleted projects/sources; or database bloated with soft-deleted records
- **Data Model Decision:** `deletedAt` column on `projects` and `sources` tables only; other entities use CASCADE hard delete
- **Proposed Resolution:** Post-MVP: add "Trash" view to recover deleted projects/sources within 30 days; permanent delete after 30 days
- **Status:** ACCEPTED (appropriate for MVP)
- **Owner:** Agent 5 (UI/UX) for trash UI design
- **Date:** 2026-01-19

---

**Document Status: COMPLETE**

**Next Agent:** Agent 4 (API Contract) will read this Data Model to design RESTful API endpoints.
